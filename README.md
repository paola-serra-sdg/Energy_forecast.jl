# Energy forecast

[![Build Status](https://github.com/paola-serra-sdg/Energy_forecast.jl/actions/workflows/CI.yml/badge.svg?branch=master)](https://github.com/paola-serra-sdg/Energy_forecast.jl/actions/workflows/CI.yml?query=branch%3Amaster)


## Background
Every year, global energy consumption continues to rise, making it imperative for energy providers to explore and develop models that can better forecast and plan for energy demand. Accurately predicting the behavior of the energy system is critical in mitigating potential uncertainties and facilitating load shaping, which can help to reduce waste.
Moreover, electric energy must be consumed at the same time it is generated in the power plant due to its physical characteristics. Deep neural networks have demonstrated their effectiveness in this regard, as they are capable of learning complex patterns and making precise predictions based on large amounts of historical energy consumption data. This, in turn, can lead to cost savings and improved energy efficiency.
Furthermore, accurate energy consumption forecasts can guide decision-making in the energy sector, ensuring a stable and reliable energy supply.

## Dataset
The dataset is composed by 255 time series about energy consumption, one per user, sampled every 15 minutes.
Data were harvested in 2017 in Guayaquil (Ecuador), capturing values for the entire year from January 1st to December 31st.
The data is presented in tabular form, consisting of seven columns, with one column indicating the timing, and the other columns representing various types of power usage:
- Timeline;
- Real or Active Power (kW), the power that is actually utilized or consumed;
- Real Power without Power Factor, that is the ratio between Real Power and Total Power;
- Reactive Power (kVA), the power that is developed in the circuit reactance;
- Reactive Power without Power Factor;
- Real Power Demand;
- Reactive Power Demand.

## Preprocessing
The ultimate objective is to predict real power demand for week 44. To achieve this goal, we conduct a thorough analysis of the data, including data cleaning and standardization. We eliminate 33 users from the analysis due to anomalies such as different time samples or a lack of measurements until the week we want to predict. The input data is generated by selecting all demand values except for the last week of the period. The output is created by shifting the input values by one week, which corresponds to the beginning of the second week up to the end. Therefore, the first week of input is not included in the output, and the last week of output is not included in the input.
The model is trained using data up to the end of the 43rd week and then evaluated on the 44th week. The reason for choosing the 44th week for evaluation is that not all users have demand values in the last weeks of the year. Finally, we split the dataset into training and testing sets.

## Architectures
**Time machine**

After experimenting with various combinations of dimensions, we determine that dividing the space into four subspaces, each with a depth of four and one with a depth of one, yielded the best results. This approach results in a total of 13 subspaces. We utilize a time block of two days. Additionally, to ensure the preservation of a full day, we incorporate a padding value of 96, which corresponds to 24 hours with 4 observations every hour. For nonlinearity, we implement the sigmoid function. The model also includes a convolutional layer with a kernel size of one, which accepts 13 channels as inputs representing the subspaces, and generates a single output channel.

**CNN-1**

The second model we have defined is a CNN consisting of 4 convolutional layers (CNN-1). The architecture is composed of a sequence of convolutional layers that are connected in a sequential manner without any shortcuts.
We carefully selected the architecture of the convolutional model to resemble the structure of the time machine so that a fair comparison could be made between the two models.
The convolutional neural network utilized for this task is a one-dimensional CNN, as the input data consists of time series.
We chose four layers because this corresponds to the number of subspaces in which the time machine is divided.
We chose a one-day filter as kernel, which consists of 24 hours with four observations every 15 minutes in an hour.
Regarding the padding setting, we added 95 zero-values to the left side (representing the past) of the input data, and zero-values to the right side (representing the future).

**CNN-2**

To make the number of parameters comparable, we attempted to increase the number of parameters in the CNN. In the second architecture version of the CNN (CNN-2), which still contains four convolutional layers, we arrange the layers hyperparameters in order to have a comparable number of model parameters with the time machine. We retain all other settings from the first convolutional model.
This change results in a total of 16923 parameters, allowing us to make a fair comparison.

## Training and testing
In order to test the generalization power of both classical and time machine models, we decided to train our models on a single user due to the high electrical demand variance across users. Even though this strategy may not capture the full complexity of the dataset, it allows us to compare the performance of all models not only on the test set associated with the selected user but also on all users. By doing so, we can obtain a more encompassing view of how parametric machines and convolutional neural networks generalize on unseen, highly variable data.

## Results and generalization
In this energy consumption forecasting problem, the results indicate that the time machine outperforms convolutional architectures. The machine's capacity to process all inputs and retain more information through the presence of shortcuts allows for better detection of fluctuations. This architecture is more effective in capturing the complex and varied patterns in the data, giving the machine an advantage over convolutional neural networks in energy consumption forecasting.



